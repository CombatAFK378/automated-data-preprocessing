import pandas as pd
import numpy as np
import re
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.pipeline import Pipeline
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score

def train_nlp_model(file_path):
    print(f"--- Starting NLP Pipeline: {file_path} ---")
    
    # 1. LOAD DATA
    try:
        if file_path.endswith('.csv'):
            df = pd.read_csv(file_path)
        elif file_path.endswith(('.xls', '.xlsx')):
            df = pd.read_excel(file_path)
        else:
            return "Error: Unsupported format"
    except Exception as e:
        return f"Error loading file: {e}"

    print(f"Loaded {len(df)} rows, {len(df.columns)} columns")

    # 2. AUTO-DETECT COLUMNS
    text_candidates = df.select_dtypes(include=['object']).columns.tolist()
    
    if not text_candidates:
        return "Error: No text column found"
        
    # Pick Text Column (Longest average length)
    text_col = max(text_candidates, key=lambda c: df[c].astype(str).str.len().mean())
    
    # Pick Label Column (Fewest unique values, excluding text col)
    label_candidates = [c for c in df.columns if c != text_col and df[c].nunique() < 100]
    
    if not label_candidates:
        return "Error: No suitable label column found"
        
    label_col = min(label_candidates, key=lambda c: df[c].nunique())
    
    print(f"Detected Text: '{text_col}' | Target: '{label_col}'")

    # 3. VALIDATE
    df = df.dropna(subset=[text_col, label_col])
    
    if len(df) < 20:
        return f"Error: Too few samples ({len(df)})"

    class_counts = df[label_col].value_counts()
    print(f"\nClass Distribution:\n{class_counts}")

    # 4. CLEAN TEXT
    print("\nCleaning text...")
    
    def clean_text(text):
        text = str(text).lower()
        # MANUAL CHECK: Ensure there is only ONE backslash before S, s, etc.
        text = re.sub(r'\S*@\S*\s?', '', text)     # Remove emails
        text = re.sub(r'http\S+', '', text)        # Remove URLs
        text = re.sub(r'[^a-z0-9\s]', '', text)    # Keep letters & numbers
        text = re.sub(r'\s+', ' ', text).strip()   # Remove extra spaces
        return text if text else "empty"

    df['clean_text'] = df[text_col].apply(clean_text)

    # 5. SPLIT
    use_stratify = class_counts.min() >= 2
    
    if use_stratify:
        print("Using stratified split...")
        X_train, X_test, y_train, y_test = train_test_split(
            df['clean_text'], df[label_col], 
            test_size=0.2, random_state=42,
            stratify=df[label_col]
        )
    else:
        print("Warning: Regular split...")
        X_train, X_test, y_train, y_test = train_test_split(
            df['clean_text'], df[label_col], 
            test_size=0.2, random_state=42
        )

    # 6. TRAIN
    print("\nTraining model...")
    
    model = Pipeline([
        ('tfidf', TfidfVectorizer(
            max_features=5000,
            ngram_range=(1, 2),
            min_df=2
        )),
        ('clf', LogisticRegression(
            max_iter=1000, 
            random_state=42, 
            class_weight='balanced'
        ))
    ])
    
    model.fit(X_train, y_train)

    # 7. EVALUATE
    print("\n--- Evaluation ---")
    y_pred = model.predict(X_test)
    
    print(f"Accuracy: {accuracy_score(y_test, y_pred):.2%}")
    print("\nClassification Report:")
    print(classification_report(y_test, y_pred, zero_division=0))
    
    return model

if __name__ == "__main__":
    # Change filename here
    #file_path = "/content/Your_File_Name.csv"  
    file_path = "dataset.csv" 
    train_nlp_model(file_path)import pandas as pd
import numpy as np
import re
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.pipeline import Pipeline
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score

def train_nlp_model(file_path):
    print(f"--- Starting NLP Pipeline: {file_path} ---")
    
    # 1. LOAD DATA
    try:
        if file_path.endswith('.csv'):
            df = pd.read_csv(file_path)
        elif file_path.endswith(('.xls', '.xlsx')):
            df = pd.read_excel(file_path)
        else:
            return "Error: Unsupported format"
    except Exception as e:
        return f"Error loading file: {e}"

    print(f"Loaded {len(df)} rows, {len(df.columns)} columns")

    # 2. AUTO-DETECT COLUMNS
    text_candidates = df.select_dtypes(include=['object']).columns.tolist()
    
    if not text_candidates:
        return "Error: No text column found"
        
    # Pick Text Column (Longest average length)
    text_col = max(text_candidates, key=lambda c: df[c].astype(str).str.len().mean())
    
    # Pick Label Column (Fewest unique values, excluding text col)
    label_candidates = [c for c in df.columns if c != text_col and df[c].nunique() < 100]
    
    if not label_candidates:
        return "Error: No suitable label column found"
        
    label_col = min(label_candidates, key=lambda c: df[c].nunique())
    
    print(f"Detected Text: '{text_col}' | Target: '{label_col}'")

    # 3. VALIDATE
    df = df.dropna(subset=[text_col, label_col])
    
    if len(df) < 20:
        return f"Error: Too few samples ({len(df)})"

    class_counts = df[label_col].value_counts()
    print(f"\nClass Distribution:\n{class_counts}")

    # 4. CLEAN TEXT
    print("\nCleaning text...")
    
    def clean_text(text):
        text = str(text).lower()
        # MANUAL CHECK: Ensure there is only ONE backslash before S, s, etc.
        text = re.sub(r'\S*@\S*\s?', '', text)     # Remove emails
        text = re.sub(r'http\S+', '', text)        # Remove URLs
        text = re.sub(r'[^a-z0-9\s]', '', text)    # Keep letters & numbers
        text = re.sub(r'\s+', ' ', text).strip()   # Remove extra spaces
        return text if text else "empty"

    df['clean_text'] = df[text_col].apply(clean_text)

    # 5. SPLIT
    use_stratify = class_counts.min() >= 2
    
    if use_stratify:
        print("Using stratified split...")
        X_train, X_test, y_train, y_test = train_test_split(
            df['clean_text'], df[label_col], 
            test_size=0.2, random_state=42,
            stratify=df[label_col]
        )
    else:
        print("Warning: Regular split...")
        X_train, X_test, y_train, y_test = train_test_split(
            df['clean_text'], df[label_col], 
            test_size=0.2, random_state=42
        )

    # 6. TRAIN
    print("\nTraining model...")
    
    model = Pipeline([
        ('tfidf', TfidfVectorizer(
            max_features=5000,
            ngram_range=(1, 2),
            min_df=2
        )),
        ('clf', LogisticRegression(
            max_iter=1000, 
            random_state=42, 
            class_weight='balanced'
        ))
    ])
    
    model.fit(X_train, y_train)

    # 7. EVALUATE
    print("\n--- Evaluation ---")
    y_pred = model.predict(X_test)
    
    print(f"Accuracy: {accuracy_score(y_test, y_pred):.2%}")
    print("\nClassification Report:")
    print(classification_report(y_test, y_pred, zero_division=0))
    
    return model

if __name__ == "__main__":
    # Change filename here
    #file_path = "/content/Your_File_Name.csv"  
    file_path = "dataset.csv" 
    train_nlp_model(file_path)
